<HTML><HEAD><TITLE>Patron Files with the Jitters</TITLE><META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset= iso-8859-1"><style>@import url(/msdn_ie4.css);</style>
<link disabled rel="stylesheet" href="/msdn_ie3.css"></HEAD><BODY bgcolor="#FFFFFF">
<font face="verdana,arial,helvetica" size="2"><FORM NAME="x"><OBJECT CLASSID="clsid:9c2ac687-ceef-11cf-96d9-00a0c903b016" NAME="iv"></OBJECT></FORM>
<H3>Patron Files with the Jitters</H3><P>Patron is designed to manage a document composed of pages, with each page eventually managing any number of tenants—bitmaps, metafiles, compound document content objects, and controls. If we were to implement Patron using traditional file I/O, we would create a file with the three primary structures listed on the following page and illustrated in Figure 7-4.</P>
<UL><LI>File (document) header: contains the document's page count, the printer configuration (a DEVMODE structure), and an offset to the first page.</LI><LI>Page: contains the tenant count, the offset of the next page, and a variable-length list of offsets to the tenants. All page structures are stored sequentially in the file before any tenant structures.</LI><LI>Tenant: contains the type of tenant, the length of tenant-specific data, and the tenant's data itself.</LI></UL><P>    <img src="f07dd04.gif"></P>
<P><B>Figure 7-4.</B></P>
<P><B>A possible Patron file with traditional file I/O.</B></P>
<P>Certainly this sort of layout is manageable, albeit tedious. To write such a file, we would first write the file header, then all the page headers, and then all the tenants. Writing the file header is simple because we know its size and can easily calculate the offset of the first page to store in this header. Before writing the page headers, however, we need to build the entire list in memory to determine the total size of the page list and store the appropriate tenant offsets in each page structure. When we have this list, we write it all out to disk at once and then write each tenant in turn. Besides a little tedium in calculating the offsets, this code would be simple enough and performance would be good—all the pages are at the front of the file, and large seeks occur only when a particular tenant is accessed.</P>
<P>Let's say now that an end user deletes a page and then saves the file again. We have two alternatives. We can rewrite the entire file (typically the easiest option), or we can mark the structure for the deleted page as "unused" and mark its tenant structures as "unused" as well. The second option would not reduce the file size but would allow a fast save. This is a highly demanded feature in today's applications, so we opt for the second choice.</P>
<P>Now the end user adds a page in the middle of the document and adds a few tenants to that page. The end user saves again, giving us the following three choices for where to put this page:</P>
<UL><LI>In "unused" space from a previous deletion</LI><LI>At the end of the file</LI><LI>In line with existing page structures, which would require rewriting the entire file</LI></UL><P>The first two options would allow an incremental fast save again, but they negate the idea that all the page structures would be stored sequentially in the file introducing the complexities of a fragmented file. The third option has the potential to be horrendously slow, especially if some tenants (a true color bitmap, for instance) have large amounts of data.</P>
<P>At this point, we perform the engineering exercise called "compromise" and weigh the possible options: we can have fast saves with file fragmentation, or we can have files efficient to read but slower than molasses to save. In a performance-driven market, we choose fast saves to get good timing reviews in trade magazines. Get a few developers to put in some overtime, and you'll have a wonderfully elaborate garbage collection scheme for managing free space in your files as best you can, just as you would handle free space in any memory manager. You would allow fast saves for performance and give the user the option of a full save, which would rewrite the file completely, effectively defragmenting the whole thing. Cool. We just turned a problem into a couple of features by writing a great deal of file management code. Yes! Gives us the satisfaction of a Double Tall Macho Grande Latte.</P>
<P>On an actual system, all of our garbage collection and defragmentation work buys us very little unless the end user defragments the hard disk. A defragmentation scheme managed by an application works on what the application sees as the contents of the file, faithfully reproduced by the file system. But that same file system probably has the actual file contents spread all over the physical disk itself. All the work we did to make the file look good has little to do with real performance on a fragmented disk: a 128-byte seek in the file might equate to a 128-MB seek on disk, and a 10-MB seek in the file might actually seek only 10 KB on disk.</P>
<P>After we've put in all the work—and compromised a number of other features we really wanted in the application—we find that our cool file management code does little more than keep the file size to a minimum. It bought us little in the way of real performance. In other words, we missed the design goal completely and the jitters set in, making us truly pay for that latte.</P></font></body></HTML>
